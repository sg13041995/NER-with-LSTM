{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER - Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "tf.keras.utils.set_random_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a dataset from Kaggle.\n",
    "\n",
    "The original data consists of four columns:\n",
    "\n",
    "- the sentence number,\n",
    "- the word,\n",
    "- the part of speech of the word\n",
    "- and the tags.\n",
    "\n",
    "A few tags are:\n",
    "\n",
    "- geo: geographical entity\n",
    "- org: organization\n",
    "- per: person\n",
    "- gpe: geopolitical entity\n",
    "- tim: time indicator\n",
    "- art: artifact\n",
    "- eve: event\n",
    "- nat: natural phenomenon\n",
    "- O: filler word\n",
    "\n",
    "The prepositions in the tags mean:\n",
    "\n",
    "- I: Token is inside an entity.\n",
    "- B: Token begins an entity.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "The tags would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O\n",
       "20          NaN           from   IN      O\n",
       "21          NaN           that   DT      O\n",
       "22          NaN        country   NN      O\n",
       "23          NaN              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data\n",
    "\n",
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"ISO-8859-1\")\n",
    "data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a preprocessed version of the data which was generated from the original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "ORIGINAL DATA:\n",
      "     Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "# Exploring the preprocessed data (loading from small group)\n",
    "\n",
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)\n",
    "\n",
    "print('ORIGINAL DATA:\\n', data.head())\n",
    "\n",
    "del (data, train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data for training validation and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding=\"ISO-8859-1\") as file:\n",
    "        data = np.array([line.strip() for line in file.readlines()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is divided into sentences and labels for each set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data data from large group\n",
    "\n",
    "train_sentences = load_data('data/large/train/sentences.txt')\n",
    "train_labels = load_data('data/large/train/labels.txt')\n",
    "\n",
    "val_sentences = load_data('data/large/val/sentences.txt')\n",
    "val_labels = load_data('data/large/val/labels.txt')\n",
    "\n",
    "test_sentences = load_data('data/large/test/sentences.txt')\n",
    "test_labels = load_data('data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33570\n",
      "7194\n",
      "7194\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'\n",
      " 'Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"'\n",
      " 'They marched from the Houses of Parliament to a rally in Hyde Park .'\n",
      " 'Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 .'\n",
      " \"The protest comes on the eve of the annual conference of Britain 's ruling Labor Party in the southern English seaside resort of Brighton .\"]\n",
      "['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O'\n",
      " 'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O B-geo I-geo O' 'O O O O O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O B-geo O O B-org I-org O O O B-gpe O O O B-geo O']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:5])\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Russia 's victory put the eight-time Olympic champions into the quarterfinals and also clinched a spot for Sweden .\"\n",
      " 'Slovakia advanced with a win over the United States ( 02-Jan ) on Saturday , leaving one remaining spot from Group-B .'\n",
      " 'China has announced its sixth human bird flu death .'\n",
      " 'Chinese health officials said Wednesday a 35-year-old woman from Sichuan province died last week from the H5N1 strain .'\n",
      " 'Meanwhile , Turkish officials say bird flu killed an 11-year-old girl on her way to a hospital Wednesday .']\n",
      "['B-geo O O O O O O O O O O O O O O O O B-org O'\n",
      " 'B-geo O O O O O O B-geo I-geo O O O O B-tim O O O O O O B-art O'\n",
      " 'B-org O O O O O O O O O'\n",
      " 'B-gpe O O O B-tim O O O O B-geo O O O O O O B-nat B-tim O'\n",
      " 'O O B-gpe O O O O O O O O O O O O O O B-tim O']\n"
     ]
    }
   ],
   "source": [
    "print(val_sentences[:5])\n",
    "print(val_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Argentina benefits from rich natural resources , a highly literate population , an export-oriented agricultural sector , and a diversified industrial base .'\n",
      " \"Although one of the world 's wealthiest countries 100 years ago , Argentina suffered during most of the 20th century from recurring economic crises , persistent fiscal and current account deficits , high inflation , mounting external debt , and capital flight .\"\n",
      " \"A severe depression , growing public and external indebtedness , and a bank run culminated in 2001 in the most serious economic , social , and political crisis in the country 's turbulent history .\"\n",
      " \"Interim President Adolfo RODRIGUEZ SAA declared a default - the largest in history - on the government 's foreign debt in December of that year , and abruptly resigned only a few days after taking office .\"\n",
      " \"His successor , Eduardo DUHALDE , announced an end to the peso 's decade-long 1-to-1 peg to the US dollar in early 2002 .\"]\n",
      "['B-geo O O O O O O O O O O O O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O O B-geo O O O O O B-tim O O O O O O O O O O O O O O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O O O O O O B-tim O O O O O O O O O O O O O O O O O O'\n",
      " 'O B-per I-per I-per I-per O O O O O O O O O O O O O O O O B-tim I-tim I-tim O O O O O O O O O O O O O'\n",
      " 'O O O B-per I-per O O O O O O O O O O O O O B-org O O O B-tim O']\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[:5])\n",
    "print(test_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `tf.keras.layers.TextVectorization`. We will explicitly pass `standardize = None`. By default, `standardize = 'lower_and_strip_punctuation'`.\n",
    "\n",
    "This means the parser will remove all punctuation and make everything lowercase. This may influence the NER task, since an upper case in the middle of a sentence may indicate an entity. The sentences in the dataset are already split into tokens, and all tokens, including punctuation, are separated by a whitespace. The punctuations are also labeled. So everything will just be split into single tokens and then mapped to a positive integer.\n",
    "\n",
    "`tf.keras.layers.TextVectorization` will also pad the sentences. But padding won't impact at all the model's output.\n",
    "\n",
    "- padding token: \"\", integer mapped: 0\n",
    "- unknown token \"[UNK]\", integer mapped: 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the object `tf.keras.layers.TextVectorization` and the appropriate parameters to build a function that inputs an array of sentences and outputs an adapted sentence vectorizer and its vocabulary list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vectorizer(sentences):\n",
    "\n",
    "    # Setting the random seed\n",
    "    tf.keras.utils.set_random_seed(33)\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    sentences (list of str): Sentences for vocabulary adaptation.\n",
    "\n",
    "    Returns:\n",
    "    sentence_vectorizer (tf.keras.layers.TextVectorization): TextVectorization layer for sentence tokenization.\n",
    "    vocab (list of str): Extracted vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define TextVectorization object with the appropriate standardize parameter\n",
    "    sentence_vectorizer = tf.keras.layers.TextVectorization(standardize=None)\n",
    "\n",
    "    # Adapt the sentence vectorization object to the given sentences\n",
    "    sentence_vectorizer.adapt(sentences)\n",
    "\n",
    "    # Get the vocabulary\n",
    "    vocab = sentence_vectorizer.get_vocabulary()\n",
    "\n",
    "    return sentence_vectorizer, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Test vocab size: 4650\n"
     ]
    }
   ],
   "source": [
    "# Using the function to get the adapted vectorizer and vocabulary on a subset of the training data\n",
    "\n",
    "test_vectorizer, test_vocab = get_sentence_vectorizer(train_sentences[:1000])\n",
    "\n",
    "print(f\"Test vocab size: {len(test_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', '.', ',', 'in', 'of', 'to', 'a', 'and', 'The', \"'s\", 'for', 'is', 'has', 'said', 'on', 'have', 'that', 'from']\n"
     ]
    }
   ],
   "source": [
    "print(test_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like learning new NLP models !\n",
      "Sentence vectorized: [ 296  314    1   59    1    1 4649]\n"
     ]
    }
   ],
   "source": [
    "# Testing the test vectorizer\n",
    "\n",
    "sentence = \"I like learning new NLP models !\"\n",
    "\n",
    "sentence_vectorized = test_vectorizer(sentence)\n",
    "\n",
    "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the sentence vectorizer adapted on the complete training data\n",
    "\n",
    "sentence_vectorizer, vocab = get_sentence_vectorizer(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 29847\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like learning new NLP models !\n",
      "Sentence vectorized: [  654  1211  6896    69     1 11044  4126]\n"
     ]
    }
   ],
   "source": [
    "# Testing the complete vectorizer on the same example\n",
    "\n",
    "sentence = \"I like learning new NLP models !\"\n",
    "\n",
    "sentence_vectorized = sentence_vectorizer(sentence)\n",
    "\n",
    "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is a bit simpler than encoding the sentences, because there are only a few tags. Also, there will be one extra tag to represent the padded token that some sentences may have included. Padding will not interfere at all in this task.\n",
    "\n",
    "There is no meaning in having an UNK token for labels and also the padding token will be another number different from 0 for the labels. So, TextVectorization is not a good choice here.\n",
    "\n",
    "We will code our own label vectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Labels: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[0]}\")\n",
    "print(f\"Labels: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function extract all the different tags in a given set of labels (all the training example set) and return us the uniqe ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(labels):\n",
    "    tag_set = set()  # Define an empty set\n",
    "    for el in labels:\n",
    "        for tag in el.split(\" \"):\n",
    "            tag_set.add(tag)\n",
    "    tag_list = list(tag_set)\n",
    "    tag_list.sort()\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
     ]
    }
   ],
   "source": [
    "tags = get_tags(train_labels)\n",
    "print(len(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generate **tag map** and **tag_map_reverse**.\n",
    "- A mapping between the tags and positive integers\n",
    "- A reverse mapping between the positive integers and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_map(tags):\n",
    "    tag_map = {}\n",
    "    tag_map_rev = {}\n",
    "    for i, tag in enumerate(tags):\n",
    "        tag_map[tag] = i\n",
    "        tag_map_rev[i] = tag\n",
    "    return tag_map, tag_map_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n",
      "{0: 'B-art', 1: 'B-eve', 2: 'B-geo', 3: 'B-gpe', 4: 'B-nat', 5: 'B-org', 6: 'B-per', 7: 'B-tim', 8: 'I-art', 9: 'I-eve', 10: 'I-geo', 11: 'I-gpe', 12: 'I-nat', 13: 'I-org', 14: 'I-per', 15: 'I-tim', 16: 'O'}\n"
     ]
    }
   ],
   "source": [
    "tag_map, tag_map_rev = make_tag_map(tags)\n",
    "\n",
    "print(tag_map)\n",
    "print(tag_map_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextVectorization has already padded the sentences, so we must ensure that the labels are properly padded as well.Tensorflow has built-in functions for padding `tf.keras.utils.pad_sequences`.\n",
    "\n",
    "We will pad the vectorized labels with the value -1. We will not use 0 to simplify loss masking and evaluation in further steps.\n",
    "\n",
    "This is because to properly classify one token, a log softmax transformation will be performed and the index with greater value will be the index label. Since index starts at 0, it is better to keep the label 0 as a valid index, even though it is possible to also use 0 as a mask value for labels, but it would require some tweaks in the model architecture or in the loss computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the label vectorizer.\n",
    "\n",
    "This function inputs a list of labels and a tag mapping and outputs their respective label ids via a tag map lookup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vectorizer(labels, tag_map, DEBUG_PRINT=False):\n",
    "    \"\"\"\n",
    "    Convert list of label strings to padded label IDs using a tag mapping.\n",
    "\n",
    "    Parameters:\n",
    "    labels (list of str): List of label strings.\n",
    "    tag_map (dict): Dictionary mapping tags to IDs.\n",
    "    Returns:\n",
    "    label_ids (numpy.ndarray): Padded array of label IDs.\n",
    "    \"\"\"\n",
    "    label_ids = []  # It can't be a numpy array yet, since each sentence has a different size\n",
    "\n",
    "    # Each element in labels is a string of tags so for each of them:\n",
    "    for element in labels:\n",
    "        # Split it into single tokens. You may use .split function for strings. Be aware to split it by a blank space!\n",
    "        tokens = element.split()\n",
    "\n",
    "        # DEBUG PRINT\n",
    "        if DEBUG_PRINT == True:\n",
    "            print(\"tokens\", tokens, \"\\n\")\n",
    "\n",
    "        # Use the dictionaty tag_map passed as an argument to the label_vectorizer function\n",
    "        # to make the correspondence between tags and numbers.\n",
    "        element_ids = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Tag map lookup\n",
    "            # Appending the ids corresponding to the tokens in the label\n",
    "            element_ids.append(tag_map[token])\n",
    "\n",
    "        # DEBUG PRINT\n",
    "        if DEBUG_PRINT == True:\n",
    "            print(\"element_ids\", element_ids, \"\\n\")\n",
    "\n",
    "        # Append the found ids to corresponding to the current element to label_ids list\n",
    "        label_ids.append(element_ids)\n",
    "\n",
    "    # DEBUG PRINT\n",
    "    if DEBUG_PRINT == True:\n",
    "        print(\"label_ids\", label_ids, \"\\n\")\n",
    "\n",
    "    # Pad the elements\n",
    "    label_ids = tf.keras.utils.pad_sequences(\n",
    "        label_ids, padding='post', value=-1)\n",
    "\n",
    "    # DEBUG PRINT\n",
    "    if DEBUG_PRINT == True:\n",
    "        print(\"label_ids\", label_ids, \"\\n\")\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a single example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: They marched from the Houses of Parliament to a rally in Hyde Park .\n",
      "Labels: O O O O O O O O O O O B-geo I-geo O\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[2]}\")\n",
    "print(f\"Labels: {train_labels[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized labels: [[16 16 16 16 16 16 16 16 16 16 16  2 10 16]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vectorized labels: {label_vectorizer([train_labels[2]], tag_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at two examples together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['They marched from the Houses of Parliament to a rally in Hyde Park .'\n",
      " 'Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 .']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[2:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O O O O O O O O O O O B-geo I-geo O' 'O O O O O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels: {train_labels[2:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized labels: [[16 16 16 16 16 16 16 16 16 16 16  2 10 16 -1]\n",
      " [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vectorized labels: {label_vectorizer(train_labels[2:4], tag_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the dataset for training, validation and testing.\n",
    "\n",
    "We will be using `tf.data.Dataset` class, which provides an optimized way to handle data to feed into a tensorflow model. It avoids keeping all the data in memory, thus it makes the training faster.\n",
    "\n",
    "We will be using the `tf.data.Dataset.from_tensor_slices` function that converts any iterable into a Tensorflow dataset.\n",
    "\n",
    "We can pass a tuple of `(sentences,labels)` and Tensorflow will understand that each sentence is mapped to its respective label, therefore it is expected that if a tuple of arrays is passed, both arrays have the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
    "    sentences_ids = sentence_vectorizer(sentences)\n",
    "    labels_ids = label_vectorizer(labels, tag_map=tag_map)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sentences_ids, labels_ids))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(\n",
    "    train_sentences, train_labels, sentence_vectorizer, tag_map)\n",
    "val_dataset = generate_dataset(\n",
    "    val_sentences, val_labels,  sentence_vectorizer, tag_map)\n",
    "test_dataset = generate_dataset(\n",
    "    test_sentences, test_labels,  sentence_vectorizer, tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of vocabulary words in the training set: 29847\n",
      "\n",
      "The training size is 33570\n",
      "The validation size is 7194\n",
      "\n",
      "An example of the first sentence is\n",
      "\t [1046    6 1121   18 1832  232  543    7  528    2  158    5   60    9\n",
      "  648    2  922    6  192   87   22   16   54    3    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "An example of its corresponding label is\n",
      "\t [16 16 16 16 16 16  2 16 16 16 16 16  2 16 16 16 16 16  3 16 16 16 16 16\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Exploring information about the training data\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "\n",
    "g_vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Num of vocabulary words in the training set: {g_vocab_size}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print('The training size is', len(train_dataset))\n",
    "print('The validation size is', len(val_dataset))\n",
    "\n",
    "print()\n",
    "\n",
    "print('An example of the first sentence is\\n\\t',\n",
    "      next(iter(train_dataset))[0].numpy())\n",
    "print('An example of its corresponding label is\\n\\t',\n",
    "      next(iter(train_dataset))[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About RNNs and LSTMs inputs\n",
    "\n",
    "Tensorflow implementation of RNNs (in particular LSTMs) allow us to pass a variable size of input sentences. However this cannot be done in the same batch. You must assure that, for each batch, the shapes for our input tensors are the same.\n",
    "\n",
    "For this purpose, the size of the padding should not influence the final result. Therefore, it does not matter if we perform the padding for each batch or in the entire dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs will be sentences represented as tensors that are fed to a model with,\n",
    "\n",
    "- An Embedding layer,\n",
    "- A LSTM layer\n",
    "- A Dense layer\n",
    "- A log softmax layer\n",
    "\n",
    "We may choose between outputting only the very last LSTM output for each sentence, but we may also request the LSTM to output every value for a sentence - this is what we want.\n",
    "\n",
    "We will need every output, because the idea is to label every token in the sentence and not to predict the next token or even make an overall classification task for that sentence.\n",
    "\n",
    "This implies that when we input a single sentence, such as `[452, 3400, 123, 0, 0, 0]`, the expected output should be an array for each word ID, with a length equal to the number of tags. This output is obtained by applying the LogSoftfmax function for each of the `len(tags)` values.\n",
    "\n",
    "So, in the case of the example array with a shape of `(6,)`, the output should be an array with a shape of `(6, len(tags))`.\n",
    "\n",
    "In our case, we've seen that each sentence in the training set is 104 values long, so in a batch of, say, 64 tensors, the model shoud input a tensor of shape `(64,104)` and output another tensor with shape `(64,104,17)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About tensorflow layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.keras.Sequential`**\n",
    "\n",
    "- This combinator applies layers serially (by function composition). It is a tensorflow model object.\n",
    "\n",
    "---\n",
    "\n",
    "**`tf.keras.layers.Embedding`**\n",
    "\n",
    "- Initializes the embedding layer `Embedding(input_dim, output_dim, mask_zero = False)`\n",
    "- `input_dim` is the expected range of integers for each tensor in the batch. Note that the `input_dim` is not related to array size, but to the possible range of integers expected in the input. Usually this is the vocabulary size, but it may differ by 1, depending on further parameters.\n",
    "- `output_dim` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example). Each word processed will be assigned an array of size `output_dim`. So if one array of shape (3,) is passed (example of such an array `[100,203,204]`), then the Embedding layer should have output shape (3,output_dim).\n",
    "- `mask_zero` is a boolean telling whether 0 is a mask value or not. If `mask_zero = True`, then some considerations must be done:\n",
    "  - The value 0 should be reserved as the mask value, as it will be ignored in training.\n",
    "  - We need to add 1 in `input_dim`, since now Tensorflow will consider that one extra 0 value may show up in each sentence.\n",
    "\n",
    "---\n",
    "\n",
    "**`tf.keras.layers.LSTM`**\n",
    "\n",
    "- `LSTM(units, return_sequences)` Builds an LSTM layer with hidden state and cell sizes equal to `units`.\n",
    "- `units`: It is the number of `LSTM` cells we will create to pass every input to. In this case, set the `units` as the Embedding `output_dim`. This is just a choice, in fact there is no static rule preventing one from choosing any amount of LSTM units.\n",
    "- `return_sequences`: A boolean, telling whether we want to return every output value from the LSTM cells. If `return_sequences = False`, then the LSTM output shape will be `(batch_size, units)`. Otherwise, it is `(batch_size, sentence_length, units)`, since there will be an output for each word in the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "**`tf.keras.layers.Dense`**\n",
    "\n",
    "- `Dense(units, activation)`\n",
    "- `units`: It is the number of units chosen for this dense layer, i.e., it is the dimensionality of the output space. In this case, each value passed through the Dense layer must be mapped into a vector with length `num_of_classes` (in this case, `len(tags)`).\n",
    "- `activation`: This is the activation that will be performed after computing the values in the Dense layer. Since the Dense layer comes before the LogSoftmax step, we can pass the LogSoftmax function as activation function here. \\*\\*We can find the implementation for LogSoftmax under `tf.nn`. So we may call it as `tf.nn.log_softmax`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(len_tags, vocab_size, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    Create a Named Entity Recognition (NER) model.\n",
    "\n",
    "    Parameters:\n",
    "    len_tags (int): Number of NER tags (output classes).\n",
    "    vocab_size (int): Vocabulary size.\n",
    "    embedding_dim (int, optional): Dimension of embedding and LSTM layers (default is 50).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): NER model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential(name='sequential')\n",
    "\n",
    "    # Add the tf.keras.layers.Embedding layer. Do not forget to mask out the zeros!\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size +\n",
    "              1, output_dim=embedding_dim, mask_zero=True))\n",
    "\n",
    "    # Add the LSTM layer. Make sure we are passing the right dimension (defined in the docstring above)\n",
    "    # and returning every output for the tf.keras.layers.LSTM layer and not the very last one.\n",
    "    model.add(tf.keras.layers.LSTM(units=embedding_dim, return_sequences=True))\n",
    "\n",
    "    # Add the final tf.keras.layers.Dense with the appropriate activation function. Remember we must pass the activation function itself and not its call!\n",
    "    # We must use tf.nn.log_softmax instead of tf.nn.log_softmax().\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=len_tags, activation=tf.nn.log_softmax))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked loss and metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Accuracy Function**\n",
    "\n",
    "Before training the model, we need to create our own function to compute the accuracy. Tensorflow has built-in accuracy metrics but we cannot pass values to be ignored. This will impact the calculations, since we must remove the padded values.\n",
    "\n",
    "Usually, the metric that inputs true labels and predicted labels and outputs how many times the predicted and true labels match is called `accuracy`.\n",
    "\n",
    "In some cases, however, there is one more step before getting the predicted labels. This may happen if, instead of passing the predicted labels, a vector of probabilities is passed.\n",
    "\n",
    "In such case, there is a need to perform an `argmax` for each prediction to find the appropriate predicted label. Such situations happen very often, therefore Tensorflow has a set of functions, with prefix `Sparse`, that performs this operation in the backend.\n",
    "\n",
    "Unfortunately, it does not provide values to ignore in the accuracy case. This is what we will work on now.\n",
    "\n",
    "Note that the model's prediction has 3 axes:\n",
    "\n",
    "- the number of examples (batch size)\n",
    "- the number of words in each example (padded to be as long as the longest sentence in the batch)\n",
    "- the number of possible targets (the 17 named entity tags).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Loss Function**\n",
    "\n",
    "Another important function is the loss function. In this case, we will use the Cross Entropy loss, but we need a multiclass implementation of it, also we may look for its `Sparse` version.\n",
    "\n",
    "Tensorflow has a SparseCategoricalCrossentropy loss function, which is already imported by the name SparseCategoricalCrossEntropy.\n",
    "\n",
    "The arguments we will need:\n",
    "\n",
    "- `from_logits`: This indicates if the values are raw values or normalized values (probabilities). Since the last layer of the model finishes with a LogSoftMax call, the results are **not** normalized - they do not lie between 0 and 1.\n",
    "- `ignore_class`: This indicates which class should be ignored when computing the crossentropy. Remember that the class related to padding value is set to be 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom masked loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the masked sparse categorical cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): True labels.\n",
    "    y_pred (tensor): Predicted logits.\n",
    "\n",
    "    Returns:\n",
    "    loss (tensor): Calculated loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss for each item in the batch. Remember to pass the right arguments, as discussed above!\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, ignore_class=-1)\n",
    "\n",
    "    # Use the previous defined function to compute the loss\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0508584, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Trying out the custom loss function\n",
    "\n",
    "true_labels = tf.constant([0, 1, 2, 0])\n",
    "\n",
    "predicted_logits = tf.constant([[0.1, 0.6, 0.3],\n",
    "                                [0.2, 0.7, 0.1],\n",
    "                                [0.1, 0.5, 0.4],\n",
    "                                [0.4, 0.4, 0.2]])\n",
    "\n",
    "print(masked_loss(true_labels, predicted_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom masked accuracy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a masked version of the accuracy function.\n",
    "\n",
    "We will need to perform an argmax to get the predicted label for each element in the batch. We need to make sure to provide the appropriate axis in the argmax function.\n",
    "\n",
    "Furthermore, remember to use only tensorflow operations.\n",
    "\n",
    "Even though numpy has every function we will need, to pass it as a loss function and/or metric function, we must use tensorflow operations, due to internal optimizations that Tensorflow performs for reliable fitting.\n",
    "\n",
    "The following tensorflow functions are already loaded in memory, so we can directly call them.\n",
    "\n",
    "- `tf.equal`, equivalent to `np.equal`\n",
    "- `tf.cast`, equivalent to `np.astype`\n",
    "- `tf.reduce_sum`, equiavalent to `np.sum`\n",
    "- `tf.math.argmax`, equivalent to `np.argmax`\n",
    "- We may need `tf.float32` while casting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate masked accuracy for predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): True labels.\n",
    "    y_pred (tensor): Predicted logits.\n",
    "\n",
    "    Returns:\n",
    "    accuracy (tensor): Masked accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    # We must always cast the tensors to the same type in order to use them in training.\n",
    "    # Since we will make divisions, it is safe to use tf.float32 data type.\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # Create the mask, i.e., the values that will be ignored\n",
    "    mask = tf.not_equal(y_true, -1)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Perform argmax to get the predicted values\n",
    "    y_pred_class = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred_class = tf.cast(y_pred_class, tf.float32)\n",
    "\n",
    "    # Compare the true values with the predicted ones\n",
    "    matches_true_pred = tf.equal(y_true, y_pred_class)\n",
    "    matches_true_pred = tf.cast(matches_true_pred, tf.float32)\n",
    "\n",
    "    # Multiply the acc tensor with the masks\n",
    "    matches_true_pred *= mask\n",
    "\n",
    "    # Compute masked accuracy\n",
    "    # quotient between the total matches and the total valid values, i.e., the amount of non-masked values\n",
    "    masked_acc = tf.reduce_sum(matches_true_pred) / \\\n",
    "        tf.maximum(tf.reduce_sum(mask), 1)\n",
    "\n",
    "    return masked_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "true_labels = [0, 1, 2, 0]\n",
    "\n",
    "predicted_logits = [[0.1, 0.6, 0.3],\n",
    "                    [0.2, 0.7, 0.1],\n",
    "                    [0.1, 0.5, 0.4],\n",
    "                    [0.4, 0.4, 0.2]]\n",
    "\n",
    "print(masked_accuracy(true_labels, predicted_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NER(len(tag_map), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1492400   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 50)          20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 17)          867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1513467 (5.77 MB)\n",
      "Trainable params: 1513467 (5.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1492400   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 50)          20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 17)          867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1513467 (5.77 MB)\n",
      "Trainable params: 1513467 (5.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check now how padding does not affect the model's output. \n",
    "\n",
    "Of course the output dimension will change. If ten zeros are added at the end of the tensor, then the resulting output dimension will have 10 more elements (more specifically, 10 more arrays of length 17 each). \n",
    "\n",
    "However, those are removed from any calculation further on, so it won't impact at all the model's performance and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding dims is needed to pass it to the model, \n",
    "# since it expects batches and not single prediction arrays\n",
    "\n",
    "x = tf.expand_dims(np.array([545, 467, 896]), axis = 0)\n",
    "    \n",
    "x_padded = tf.expand_dims(np.array([545, 467, 896, 0, 0, 0]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1, 3, 17)\n",
      "x_padded shape: (1, 6, 17)\n"
     ]
    }
   ],
   "source": [
    "pred_x = model(x)\n",
    "pred_x_padded = model(x_padded)\n",
    "\n",
    "print(f'x shape: {pred_x.shape}\\nx_padded shape: {pred_x_padded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the last three elements of `pred_x_padded` are removed, both `pred_x` and `pred_x_padded[:3]` must have the same elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(pred_x, pred_x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one last check: let's see that both `pred_x` and `pred_x_padded` return the same loss and accuracy values. \n",
    "\n",
    "For that, we will need a `y_true` and `y_true_padded` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_loss is the same: True\n",
      "masked_accuracy is the same: True\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.expand_dims([16, 6, 12], axis = 0)\n",
    "\n",
    "# Remember we mapped the padded values to -1 in the labels\n",
    "y_true_padded = tf.expand_dims([16,6,12,-1,-1,-1], axis = 0) \n",
    "\n",
    "print(f\"masked_loss is the same: {np.allclose(masked_loss(y_true,pred_x), masked_loss(y_true_padded,pred_x_padded))}\")\n",
    "\n",
    "print(f\"masked_accuracy is the same: {np.allclose(masked_accuracy(y_true,pred_x), masked_accuracy(y_true_padded,pred_x_padded))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compile the model as follows:\n",
    "\n",
    "- Use the Adam optimizer to compute the stochastic gradient descent, with learning rate 0.01\n",
    "- Use the loss function `masked_loss` as loss function,\n",
    "- As evaluation metrics, we will use both masked_loss and masked_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss=masked_loss,\n",
    "              metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model with `shuffle = True`, over 2 epochs and passing the validation dataset as `validation_data`.\n",
    "\n",
    "We will run into an error if we just pass the datasets as they are right now, because they are not prepared in batches. We must use the method `.batch` that returns a dataset already divided in batches.\n",
    "\n",
    "The fitting takes about 1 minute to run. Only the first epoch is slow, the following ones are much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch, Batch size = 2, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "525/525 [==============================] - 79s 138ms/step - loss: 0.2687 - masked_accuracy: 0.9303 - val_loss: 0.1397 - val_masked_accuracy: 0.9585\n",
      "Epoch 2/2\n",
      "525/525 [==============================] - 77s 147ms/step - loss: 0.1099 - masked_accuracy: 0.9657 - val_loss: 0.1361 - val_masked_accuracy: 0.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x230dccb17b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting again a random seed to ensure reproducibility\n",
    "tf.keras.utils.set_random_seed(33)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.fit(train_dataset.batch(BATCH_SIZE),\n",
    "          validation_data = val_dataset.batch(BATCH_SIZE),\n",
    "          shuffle=True,\n",
    "          epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model slightly overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]\n",
      " ...\n",
      " [ 3 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]\n",
      " [16 16 16 ... -1 -1 -1]] \n",
      "\n",
      "225/225 [==============================] - 7s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "# Convert the sentences into ids\n",
    "test_sentences_id = sentence_vectorizer(test_sentences)\n",
    "\n",
    "# Convert the labels into token ids\n",
    "test_labels_id = label_vectorizer(test_labels,tag_map)\n",
    "\n",
    "print(test_labels_id, \"\\n\")\n",
    "\n",
    "# Rename to prettify next function call\n",
    "y_true = test_labels_id\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_sentences_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy in test set is: 0.9585\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model's accuracy in test set is: {masked_accuracy(y_true,y_pred).numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with single sentence string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23,)\n",
      "tf.Tensor(\n",
      "[ 1633  2112    22  2968   778  1300     4     8  2102     1   964     4\n",
      "    28 21100  1572   694     4     9     8  9656  2019   556     3], shape=(23,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test_sentences_id = sentence_vectorizer(test_sentences[0])\n",
    "print(test_sentences_id.shape)\n",
    "print(test_sentences_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 616, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm' (type LSTM):\n       inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n       mask=tf.Tensor(shape=<unknown>, dtype=bool)\n       training=False\n       initial_state=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sentences_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileh3uzgjtf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 616, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm' (type LSTM):\n       inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n       mask=tf.Tensor(shape=<unknown>, dtype=bool)\n       training=False\n       initial_state=None\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sentences_id)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to expand dimension before prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23,)\n",
      "tf.Tensor(\n",
      "[ 1633  2112    22  2968   778  1300     4     8  2102     1   964     4\n",
      "    28 21100  1572   694     4     9     8  9656  2019   556     3], shape=(23,), dtype=int64)\n",
      "(1, 23)\n",
      "tf.Tensor(\n",
      "[[ 1633  2112    22  2968   778  1300     4     8  2102     1   964     4\n",
      "     28 21100  1572   694     4     9     8  9656  2019   556     3]], shape=(1, 23), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test_sentences_id = sentence_vectorizer(test_sentences[0])\n",
    "print(test_sentences_id.shape)\n",
    "print(test_sentences_id)\n",
    "\n",
    "test_sentences_id = tf.expand_dims(test_sentences_id, axis=0)\n",
    "print(test_sentences_id.shape)\n",
    "print(test_sentences_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[[[-6.13779688e+00 -6.79265404e+00 -3.94498527e-01 -2.49608350e+00\n",
      "   -7.98183680e+00 -1.91077375e+00 -4.73243332e+00 -4.50187969e+00\n",
      "   -6.63349342e+00 -7.98887300e+00 -3.25325513e+00 -6.39309454e+00\n",
      "   -8.42420769e+00 -5.00940895e+00 -6.00375175e+00 -6.55422211e+00\n",
      "   -3.94616866e+00]\n",
      "  [-8.59139919e+00 -8.20378208e+00 -7.79352379e+00 -9.53281593e+00\n",
      "   -7.19473076e+00 -8.50618076e+00 -6.51852703e+00 -4.43424845e+00\n",
      "   -6.59590054e+00 -7.35481358e+00 -7.44420767e+00 -8.73978138e+00\n",
      "   -6.56122112e+00 -6.78192854e+00 -5.43081665e+00 -5.42684507e+00\n",
      "   -2.97499523e-02]\n",
      "  [-1.32201519e+01 -1.31385546e+01 -9.45972919e+00 -1.25458879e+01\n",
      "   -1.26733685e+01 -1.05976772e+01 -1.40467691e+01 -4.20205116e+00\n",
      "   -1.19679422e+01 -1.45431309e+01 -1.28799782e+01 -1.55161581e+01\n",
      "   -1.30081711e+01 -1.34321537e+01 -1.34954643e+01 -7.04925346e+00\n",
      "   -1.60904229e-02]\n",
      "  [-1.20176935e+01 -1.20625668e+01 -1.00448771e+01 -1.63060589e+01\n",
      "   -1.08135271e+01 -9.82305431e+00 -9.71846962e+00 -8.18900585e+00\n",
      "   -1.27153177e+01 -1.42166901e+01 -1.54240084e+01 -1.87685604e+01\n",
      "   -1.35747643e+01 -1.36139441e+01 -1.31351023e+01 -9.91365147e+00\n",
      "   -5.25455631e-04]\n",
      "  [-1.34628153e+01 -1.30724277e+01 -1.21189432e+01 -1.63817253e+01\n",
      "   -1.14815607e+01 -1.28499699e+01 -1.48654251e+01 -8.81964493e+00\n",
      "   -1.30393124e+01 -1.51438780e+01 -1.58809404e+01 -1.92350216e+01\n",
      "   -1.38485975e+01 -1.47896328e+01 -1.43900175e+01 -1.06953897e+01\n",
      "   -1.97271904e-04]\n",
      "  [-1.11771545e+01 -1.08030910e+01 -1.06279325e+01 -1.57152548e+01\n",
      "   -9.67205715e+00 -1.04387808e+01 -1.08157911e+01 -8.37775421e+00\n",
      "   -1.22740088e+01 -1.33826590e+01 -1.59646292e+01 -1.85207729e+01\n",
      "   -1.30004988e+01 -1.37029085e+01 -1.36953745e+01 -9.49327278e+00\n",
      "   -4.87327983e-04]\n",
      "  [-1.56053190e+01 -1.49790258e+01 -1.40858231e+01 -1.95870590e+01\n",
      "   -1.42860813e+01 -1.15965223e+01 -1.31104956e+01 -1.15195684e+01\n",
      "   -1.44652843e+01 -1.67126503e+01 -1.70577564e+01 -2.21536789e+01\n",
      "   -1.62583256e+01 -1.33578882e+01 -1.47975264e+01 -1.03928738e+01\n",
      "   -5.63844005e-05]\n",
      "  [-1.25379667e+01 -1.21372814e+01 -9.51940060e+00 -1.59845371e+01\n",
      "   -1.08783092e+01 -1.06799164e+01 -1.28297234e+01 -1.05374851e+01\n",
      "   -1.23757706e+01 -1.48047409e+01 -1.52947617e+01 -1.88997993e+01\n",
      "   -1.43259592e+01 -1.32015953e+01 -1.43585281e+01 -1.20005741e+01\n",
      "   -1.67594219e-04]\n",
      "  [-1.17877474e+01 -1.15734854e+01 -1.13872824e+01 -1.71811771e+01\n",
      "   -1.03555784e+01 -1.05541916e+01 -1.20108204e+01 -9.94169235e+00\n",
      "   -1.23198099e+01 -1.39235811e+01 -1.45978336e+01 -1.93019867e+01\n",
      "   -1.35401669e+01 -1.29699163e+01 -1.34446106e+01 -1.16657295e+01\n",
      "   -1.59966075e-04]\n",
      "  [-6.01726198e+00 -6.64211512e+00 -4.86121321e+00 -9.59924507e+00\n",
      "   -5.81104469e+00 -4.60171938e+00 -4.60643864e+00 -7.25911283e+00\n",
      "   -9.08309841e+00 -1.03293228e+01 -1.18378658e+01 -1.43034678e+01\n",
      "   -1.06080437e+01 -9.08455086e+00 -9.72380066e+00 -8.97831535e+00\n",
      "   -3.64021100e-02]\n",
      "  [-1.38941269e+01 -1.40728703e+01 -1.25846605e+01 -1.98540154e+01\n",
      "   -1.26685448e+01 -1.06249447e+01 -8.70426083e+00 -1.16255913e+01\n",
      "   -1.18440151e+01 -1.42387676e+01 -1.30056992e+01 -1.96466827e+01\n",
      "   -1.43074989e+01 -8.90683174e+00 -9.81088352e+00 -1.09066830e+01\n",
      "   -4.26797400e-04]\n",
      "  [-1.48946075e+01 -1.56591969e+01 -1.27019882e+01 -2.03008842e+01\n",
      "   -1.36943474e+01 -1.09142227e+01 -1.04761829e+01 -1.25611258e+01\n",
      "   -1.37522697e+01 -1.66227341e+01 -1.60278492e+01 -2.22388573e+01\n",
      "   -1.63048954e+01 -1.12444277e+01 -1.28690395e+01 -1.26344728e+01\n",
      "   -7.48606326e-05]\n",
      "  [-1.23871498e+01 -1.25772552e+01 -1.07734804e+01 -1.70762844e+01\n",
      "   -1.04972095e+01 -1.14045620e+01 -1.21310654e+01 -9.80541515e+00\n",
      "   -1.26353416e+01 -1.48290062e+01 -1.57312679e+01 -1.95526123e+01\n",
      "   -1.39664669e+01 -1.38267918e+01 -1.37362795e+01 -1.23462830e+01\n",
      "   -1.38988384e-04]\n",
      "  [-1.11705770e+01 -1.11338625e+01 -9.91451740e+00 -1.53861437e+01\n",
      "   -1.00011377e+01 -9.60904884e+00 -1.10654526e+01 -8.49265194e+00\n",
      "   -1.24314156e+01 -1.39557915e+01 -1.48069162e+01 -1.84562111e+01\n",
      "   -1.32641058e+01 -1.31560411e+01 -1.31845064e+01 -1.05325060e+01\n",
      "   -4.48960549e-04]\n",
      "  [-1.28668642e+01 -1.26744366e+01 -1.17043028e+01 -1.74246788e+01\n",
      "   -1.13268499e+01 -1.12479925e+01 -1.24548998e+01 -9.99373245e+00\n",
      "   -1.30181904e+01 -1.49534979e+01 -1.54534349e+01 -1.97665482e+01\n",
      "   -1.41832285e+01 -1.39872017e+01 -1.41907740e+01 -1.14789457e+01\n",
      "   -1.03945094e-04]\n",
      "  [-1.20934334e+01 -1.21237755e+01 -9.63895607e+00 -1.52695751e+01\n",
      "   -1.11601973e+01 -1.05814514e+01 -1.17765684e+01 -6.86851835e+00\n",
      "   -1.34398975e+01 -1.48829803e+01 -1.54999332e+01 -1.87694778e+01\n",
      "   -1.40599651e+01 -1.50694485e+01 -1.46507235e+01 -9.91731644e+00\n",
      "   -1.21722009e-03]\n",
      "  [-1.52788296e+01 -1.42623596e+01 -1.41277237e+01 -1.92272911e+01\n",
      "   -1.41297665e+01 -1.12707109e+01 -1.30980349e+01 -1.13513260e+01\n",
      "   -1.40751257e+01 -1.59145374e+01 -1.64607220e+01 -2.16198463e+01\n",
      "   -1.59428177e+01 -1.33346539e+01 -1.50416870e+01 -9.05771542e+00\n",
      "   -1.48404550e-04]\n",
      "  [-1.16097517e+01 -1.05290108e+01 -1.03660870e+01 -1.58978863e+01\n",
      "   -9.90225506e+00 -9.00048256e+00 -9.37968540e+00 -9.17974472e+00\n",
      "   -1.21260920e+01 -1.34283113e+01 -1.63968163e+01 -1.83546047e+01\n",
      "   -1.35977736e+01 -1.20583115e+01 -1.40687056e+01 -9.19666290e+00\n",
      "   -5.44638082e-04]\n",
      "  [-1.31046019e+01 -1.28382416e+01 -1.07145271e+01 -1.67212696e+01\n",
      "   -1.16203403e+01 -1.13466215e+01 -1.40621233e+01 -1.14992733e+01\n",
      "   -1.25161695e+01 -1.48791170e+01 -1.47616310e+01 -1.94449577e+01\n",
      "   -1.46053734e+01 -1.30980196e+01 -1.47103901e+01 -1.21980305e+01\n",
      "   -7.10462118e-05]\n",
      "  [-7.86590528e+00 -7.76997423e+00 -8.50370884e+00 -1.30839825e+01\n",
      "   -6.57204723e+00 -7.56287622e+00 -7.61069155e+00 -7.51466227e+00\n",
      "   -1.04404335e+01 -1.09272041e+01 -1.41748285e+01 -1.61022243e+01\n",
      "   -1.09608526e+01 -1.15548964e+01 -1.18870144e+01 -8.79743671e+00\n",
      "   -4.21089213e-03]\n",
      "  [-1.44433880e+01 -1.42425928e+01 -1.03384409e+01 -1.76138229e+01\n",
      "   -1.35474777e+01 -9.93701077e+00 -1.58213186e+01 -1.45170555e+01\n",
      "   -1.26661463e+01 -1.56967478e+01 -1.31786804e+01 -1.97715855e+01\n",
      "   -1.56990156e+01 -1.14414444e+01 -1.36818552e+01 -1.36072741e+01\n",
      "   -1.02395534e-04]\n",
      "  [-1.13373632e+01 -1.14027205e+01 -6.97707987e+00 -1.23052444e+01\n",
      "   -1.04369888e+01 -1.03388653e+01 -1.39511642e+01 -9.54582596e+00\n",
      "   -1.10643387e+01 -1.35778971e+01 -1.18086500e+01 -1.59390297e+01\n",
      "   -1.34704971e+01 -1.12261486e+01 -1.47053499e+01 -1.15407925e+01\n",
      "   -1.14470744e-03]\n",
      "  [-1.45168638e+01 -1.43096714e+01 -1.32253351e+01 -1.96391277e+01\n",
      "   -1.30887384e+01 -1.20315313e+01 -1.43732557e+01 -1.16882792e+01\n",
      "   -1.35499077e+01 -1.59691200e+01 -1.53146992e+01 -2.15181141e+01\n",
      "   -1.55133896e+01 -1.38982611e+01 -1.48432522e+01 -1.31033716e+01\n",
      "   -2.50336379e-05]]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sentences_id)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]]\n"
     ]
    }
   ],
   "source": [
    "outputs = np.argmax(y_pred, axis=-1)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\n"
     ]
    }
   ],
   "source": [
    "outputs = outputs[0]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with list of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 43)\n",
      "tf.Tensor(\n",
      "[[ 1633  2112    22  2968   778  1300     4     8  2102     1   964     4\n",
      "     28 21100  1572   694     4     9     8  9656  2019   556     3     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [ 2086    65     6     2   146    11  9084   137   704   112   350     4\n",
      "   1633  1223    96   217     6     2  2866   880    22 18793   173  8530\n",
      "      4  6843  1767     9   824  2195  5217     4   353  1838     4  5145\n",
      "   3090   939     4     9   148  2280     3]], shape=(2, 43), dtype=int64)\n",
      "======================================================================\n",
      "(1, 2, 43)\n",
      "tf.Tensor(\n",
      "[[[ 1633  2112    22  2968   778  1300     4     8  2102     1   964\n",
      "       4    28 21100  1572   694     4     9     8  9656  2019   556\n",
      "       3     0     0     0     0     0     0     0     0     0     0\n",
      "       0     0     0     0     0     0     0     0     0     0]\n",
      "  [ 2086    65     6     2   146    11  9084   137   704   112   350\n",
      "       4  1633  1223    96   217     6     2  2866   880    22 18793\n",
      "     173  8530     4  6843  1767     9   824  2195  5217     4   353\n",
      "    1838     4  5145  3090   939     4     9   148  2280     3]]], shape=(1, 2, 43), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test_sentences_id = sentence_vectorizer(test_sentences[0:2])\n",
    "print(test_sentences_id.shape)\n",
    "print(test_sentences_id)\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_sentences_id = tf.expand_dims(test_sentences_id, axis=0)\n",
    "print(test_sentences_id.shape)\n",
    "print(test_sentences_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 616, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm' (type LSTM):\n       inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n       mask=tf.Tensor(shape=<unknown>, dtype=bool)\n       training=False\n       initial_state=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sentences_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileh3uzgjtf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py\", line 616, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm' (type LSTM):\n       inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n       mask=tf.Tensor(shape=<unknown>, dtype=bool)\n       training=False\n       initial_state=None\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sentences_id)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multiple sentences, the output of `test_sentences_id = sentence_vectorizer(test_sentences[0:2])` itself becomes 2D.\n",
    "\n",
    "So, we need not to expand the dimension further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 43)\n",
      "tf.Tensor(\n",
      "[[ 1633  2112    22  2968   778  1300     4     8  2102     1   964     4\n",
      "     28 21100  1572   694     4     9     8  9656  2019   556     3     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [ 2086    65     6     2   146    11  9084   137   704   112   350     4\n",
      "   1633  1223    96   217     6     2  2866   880    22 18793   173  8530\n",
      "      4  6843  1767     9   824  2195  5217     4   353  1838     4  5145\n",
      "   3090   939     4     9   148  2280     3]], shape=(2, 43), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test_sentences_id = sentence_vectorizer(test_sentences[0:2])\n",
    "print(test_sentences_id.shape)\n",
    "print(test_sentences_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n",
      "[[[-6.1377969e+00 -6.7926545e+00 -3.9449853e-01 ... -6.0037522e+00\n",
      "   -6.5542226e+00 -3.9461689e+00]\n",
      "  [-8.5913982e+00 -8.2037811e+00 -7.7935233e+00 ... -5.4308162e+00\n",
      "   -5.4268446e+00 -2.9749952e-02]\n",
      "  [-1.3220152e+01 -1.3138554e+01 -9.4597282e+00 ... -1.3495462e+01\n",
      "   -7.0492525e+00 -1.6090538e-02]\n",
      "  ...\n",
      "  [-1.4516864e+01 -1.4309671e+01 -1.3225335e+01 ... -1.4843252e+01\n",
      "   -1.3103371e+01 -2.5033638e-05]\n",
      "  [-1.4516864e+01 -1.4309671e+01 -1.3225335e+01 ... -1.4843252e+01\n",
      "   -1.3103371e+01 -2.5033638e-05]\n",
      "  [-1.4516863e+01 -1.4309671e+01 -1.3225335e+01 ... -1.4843252e+01\n",
      "   -1.3103372e+01 -2.5033638e-05]]\n",
      "\n",
      " [[-1.1578687e+01 -1.1515079e+01 -1.1490126e+01 ... -1.0707664e+01\n",
      "   -8.5873919e+00 -6.9320016e-04]\n",
      "  [-1.0447474e+01 -1.0448552e+01 -9.0334644e+00 ... -1.3734333e+01\n",
      "   -6.8000216e+00 -4.0983647e-02]\n",
      "  [-1.1205212e+01 -9.6078587e+00 -1.0202290e+01 ... -1.4752798e+01\n",
      "   -4.3839302e+00 -1.4190835e-02]\n",
      "  ...\n",
      "  [-1.1922279e+01 -1.1801693e+01 -1.1760521e+01 ... -1.4019400e+01\n",
      "   -1.0701307e+01 -2.1360023e-04]\n",
      "  [-1.3394446e+01 -1.3048615e+01 -1.2135535e+01 ... -1.4972480e+01\n",
      "   -1.0895611e+01 -1.4006111e-04]\n",
      "  [-1.4786988e+01 -1.4389871e+01 -1.3240011e+01 ... -1.4674431e+01\n",
      "   -1.2731425e+01 -2.4079987e-05]]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sentences_id)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      "  16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]\n",
      " [16 16 16 16 16 16 16 16 16 16 16 16  2 16 16 16 16 16  7 15 15 15 16 16\n",
      "  16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16]]\n"
     ]
    }
   ],
   "source": [
    "outputs = np.argmax(y_pred, axis=-1)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting labels from label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 B-geo\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n",
      "16 O\n"
     ]
    }
   ],
   "source": [
    "# Tags for the first output\n",
    "# Converting numpy object into list as they are not iterable\n",
    "\n",
    "for i in list(outputs[0]):\n",
    "    print(i, tag_map_rev[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, model, sentence_vectorizer, tag_map):\n",
    "    \"\"\"\n",
    "    Predict NER labels for a given sentence using a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): Input sentence.\n",
    "    model (tf.keras.Model): Trained NER model.\n",
    "    sentence_vectorizer (tf.keras.layers.TextVectorization): Sentence vectorization layer.\n",
    "    tag_map (dict): Dictionary mapping tag IDs to labels.\n",
    "\n",
    "    Returns:\n",
    "    predictions (list): Predicted NER labels for the sentence.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the sentence into ids\n",
    "    sentence_vectorized = sentence_vectorizer([sentence])\n",
    "    \n",
    "    # Expand its dimension to make it appropriate to pass to the model\n",
    "    # As we are passing the single sentence inside a list itself `sentence_vectorizer([sentence])`,\n",
    "    # we need not to perform the `expand_dims` operation \n",
    "    # sentence_vectorized = tf.expand_dims(sentence_vectorized, axis=0)\n",
    "\n",
    "    # Get the model output\n",
    "    output = model.predict(sentence_vectorized)\n",
    "    \n",
    "    # Get the predicted labels for each token, using argmax function and specifying the correct axis to perform the argmax\n",
    "    outputs = np.argmax(output, axis=-1)\n",
    "    \n",
    "    # Next line is just to adjust outputs dimension. Since this function expects only one input to get a prediction, outputs will be something like [[1,2,3]]\n",
    "    # so to avoid heavy notation below, let's transform it into [1,2,3]\n",
    "    outputs = outputs[0]\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    # Iterating over every predicted token in outputs list\n",
    "    for tag_idx in outputs:\n",
    "        pred_label = tag_map_rev[tag_idx]\n",
    "        pred.append(pred_label)\n",
    "      \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "['O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-tim'] \n",
      "\n",
      "French B-gpe\n",
      "Morocco B-geo\n",
      "summer B-tim\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
    "\n",
    "predictions = predict(sentence, model, sentence_vectorizer, tag_map)\n",
    "\n",
    "print(predictions, \"\\n\")\n",
    "\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "['B-per', 'I-per', 'O', 'O', 'B-geo', 'O', 'B-tim'] \n",
      "\n",
      "Sharon B-per\n",
      "Floyd I-per\n",
      "Miami B-geo\n",
      "Friday B-tim\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
    "\n",
    "predictions = predict(sentence, model, sentence_vectorizer, tag_map)\n",
    "\n",
    "print(predictions, \"\\n\")\n",
    "\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "['B-per', 'I-per', 'O', 'O', 'B-org', 'I-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O', 'O', 'B-org', 'I-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] \n",
      "\n",
      "Peter B-per\n",
      "Parker I-per\n",
      "White B-org\n",
      "House I-org\n",
      "U.S B-org\n",
      "Sunday B-tim\n",
      "morning I-tim\n",
      "White B-org\n",
      "House I-org\n"
     ]
    }
   ],
   "source": [
    "# New york times news:\n",
    "\n",
    "sentence = \"Peter Parker , the White House director of trade and manufacturing policy of U.S , said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall , though he said it wouldn t necessarily come\"\n",
    "\n",
    "predictions = predict(sentence, model, sentence_vectorizer, tag_map)\n",
    "\n",
    "print(predictions, \"\\n\")\n",
    "\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model and load it again to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/ner_trained.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('./model/ner_trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.keras` is for the current versions of keras.\n",
    "\n",
    "For legacy support we can use `.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'Dense' using config={'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 17, 'activation': {'module': 'builtins', 'class_name': 'function', 'config': 'log_softmax_v2', 'registered_name': 'function'}, 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}.\n\nException encountered: Unknown activation function '{'module': 'builtins', 'class_name': 'function', 'config': 'log_softmax_v2', 'registered_name': 'function'}' cannot be deserialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:868\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:125\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived an invalid value for `units`, expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma positive integer. Received: units=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m \u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias \u001b[38;5;241m=\u001b[39m use_bias\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\activations.py:691\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    686\u001b[0m     use_legacy_format \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m identifier\n\u001b[0;32m    688\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     )\n\u001b[1;32m--> 691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_legacy_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(identifier):\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\activations.py:646\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects, use_legacy_format)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(returned_fn, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 646\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown activation function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot be deserialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    648\u001b[0m     )\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returned_fn\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown activation function '{'module': 'builtins', 'class_name': 'function', 'config': 'log_softmax_v2', 'registered_name': 'function'}' cannot be deserialized.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model/ner_trained.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         )\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 246\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\sequential.py:466\u001b[0m, in \u001b[0;36mSequential.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_config \u001b[38;5;129;01min\u001b[39;00m layer_configs:\n\u001b[0;32m    465\u001b[0m     use_legacy_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_config\n\u001b[1;32m--> 466\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_legacy_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(layer)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39minputs\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m build_input_shape\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(build_input_shape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m    477\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\serialization.py:276\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects, use_legacy_format)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_legacy_format:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_serialization\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m    270\u001b[0m         config,\n\u001b[0;32m    271\u001b[0m         module_objects\u001b[38;5;241m=\u001b[39mLOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS,\n\u001b[0;32m    272\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    273\u001b[0m         printable_module_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:609\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_objects[config], types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m    603\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m    604\u001b[0m                 serialize_with_public_fn(\n\u001b[0;32m    605\u001b[0m                     module_objects[config], config, fn_module_name\n\u001b[0;32m    606\u001b[0m                 ),\n\u001b[0;32m    607\u001b[0m                 custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    608\u001b[0m             )\n\u001b[1;32m--> 609\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m            \u001b[49m\u001b[43mserialize_with_public_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_config\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:870\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    873\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'Dense' using config={'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 17, 'activation': {'module': 'builtins', 'class_name': 'function', 'config': 'log_softmax_v2', 'registered_name': 'function'}, 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}.\n\nException encountered: Unknown activation function '{'module': 'builtins', 'class_name': 'function', 'config': 'log_softmax_v2', 'registered_name': 'function'}' cannot be deserialized."
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('./model/ner_trained.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
