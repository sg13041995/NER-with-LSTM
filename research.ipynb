{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "tf.keras.utils.set_random_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a dataset from Kaggle.\n",
    "\n",
    "The original data consists of four columns: \n",
    "- the sentence number, \n",
    "- the word, \n",
    "- the part of speech of the word  \n",
    "- and the tags. \n",
    "\n",
    "A few tags are: \n",
    "- geo: geographical entity\n",
    "- org: organization\n",
    "- per: person \n",
    "- gpe: geopolitical entity\n",
    "- tim: time indicator\n",
    "- art: artifact\n",
    "- eve: event\n",
    "- nat: natural phenomenon\n",
    "- O: filler word\n",
    "\n",
    "The prepositions in the tags mean:\n",
    "- I: Token is inside an entity.\n",
    "- B: Token begins an entity.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "The tags would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O\n",
       "20          NaN           from   IN      O\n",
       "21          NaN           that   DT      O\n",
       "22          NaN        country   NN      O\n",
       "23          NaN              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data\n",
    "\n",
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding = \"ISO-8859-1\")\n",
    "data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a preprocessed version of the data which was generated from the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "ORIGINAL DATA:\n",
      "     Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "# Exploring the preprocessed data (loading from small group)\n",
    "\n",
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)\n",
    "\n",
    "print('ORIGINAL DATA:\\n', data.head())\n",
    "\n",
    "del(data, train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data for training validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path,'r', encoding = \"ISO-8859-1\") as file:\n",
    "        data = np.array([line.strip() for line in file.readlines()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is divided into sentences and labels for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data data from large group\n",
    "\n",
    "train_sentences = load_data('data/large/train/sentences.txt')\n",
    "train_labels = load_data('data/large/train/labels.txt')\n",
    "\n",
    "val_sentences = load_data('data/large/val/sentences.txt')\n",
    "val_labels = load_data('data/large/val/labels.txt')\n",
    "\n",
    "test_sentences = load_data('data/large/test/sentences.txt')\n",
    "test_labels = load_data('data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33570\n",
      "7194\n",
      "7194\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'\n",
      " 'Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"'\n",
      " 'They marched from the Houses of Parliament to a rally in Hyde Park .'\n",
      " 'Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 .'\n",
      " \"The protest comes on the eve of the annual conference of Britain 's ruling Labor Party in the southern English seaside resort of Brighton .\"]\n",
      "['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O'\n",
      " 'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O B-geo I-geo O' 'O O O O O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O B-geo O O B-org I-org O O O B-gpe O O O B-geo O']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:5])\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Russia 's victory put the eight-time Olympic champions into the quarterfinals and also clinched a spot for Sweden .\"\n",
      " 'Slovakia advanced with a win over the United States ( 02-Jan ) on Saturday , leaving one remaining spot from Group-B .'\n",
      " 'China has announced its sixth human bird flu death .'\n",
      " 'Chinese health officials said Wednesday a 35-year-old woman from Sichuan province died last week from the H5N1 strain .'\n",
      " 'Meanwhile , Turkish officials say bird flu killed an 11-year-old girl on her way to a hospital Wednesday .']\n",
      "['B-geo O O O O O O O O O O O O O O O O B-org O'\n",
      " 'B-geo O O O O O O B-geo I-geo O O O O B-tim O O O O O O B-art O'\n",
      " 'B-org O O O O O O O O O'\n",
      " 'B-gpe O O O B-tim O O O O B-geo O O O O O O B-nat B-tim O'\n",
      " 'O O B-gpe O O O O O O O O O O O O O O B-tim O']\n"
     ]
    }
   ],
   "source": [
    "print(val_sentences[:5])\n",
    "print(val_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Argentina benefits from rich natural resources , a highly literate population , an export-oriented agricultural sector , and a diversified industrial base .'\n",
      " \"Although one of the world 's wealthiest countries 100 years ago , Argentina suffered during most of the 20th century from recurring economic crises , persistent fiscal and current account deficits , high inflation , mounting external debt , and capital flight .\"\n",
      " \"A severe depression , growing public and external indebtedness , and a bank run culminated in 2001 in the most serious economic , social , and political crisis in the country 's turbulent history .\"\n",
      " \"Interim President Adolfo RODRIGUEZ SAA declared a default - the largest in history - on the government 's foreign debt in December of that year , and abruptly resigned only a few days after taking office .\"\n",
      " \"His successor , Eduardo DUHALDE , announced an end to the peso 's decade-long 1-to-1 peg to the US dollar in early 2002 .\"]\n",
      "['B-geo O O O O O O O O O O O O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O O B-geo O O O O O B-tim O O O O O O O O O O O O O O O O O O O O O O O O'\n",
      " 'O O O O O O O O O O O O O O O O B-tim O O O O O O O O O O O O O O O O O O'\n",
      " 'O B-per I-per I-per I-per O O O O O O O O O O O O O O O O B-tim I-tim I-tim O O O O O O O O O O O O O'\n",
      " 'O O O B-per I-per O O O O O O O O O O O O O B-org O O O B-tim O']\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[:5])\n",
    "print(test_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `tf.keras.layers.TextVectorization`. We will explicitly pass `standardize = None`. By default, `standardize = 'lower_and_strip_punctuation'`. \n",
    "\n",
    "This means the parser will remove all punctuation and make everything lowercase. This may influence the NER task, since an upper case in the middle of a sentence may indicate an entity. The sentences in the dataset are already split into tokens, and all tokens, including punctuation, are separated by a whitespace. The punctuations are also labeled. So everything will just be split into single tokens and then mapped to a positive integer.\n",
    "\n",
    "`tf.keras.layers.TextVectorization` will also pad the sentences. But padding won't impact at all the model's output.\n",
    "\n",
    "- padding token: \"\", integer mapped: 0\n",
    "- unknown token \"[UNK]\", integer mapped: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the object `tf.keras.layers.TextVectorization` and the appropriate parameters to build a function that inputs an array of sentences and outputs an adapted sentence vectorizer and its vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vectorizer(sentences):\n",
    "    \n",
    "    # Setting the random seed\n",
    "    tf.keras.utils.set_random_seed(33)\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    sentences (list of str): Sentences for vocabulary adaptation.\n",
    "\n",
    "    Returns:\n",
    "    sentence_vectorizer (tf.keras.layers.TextVectorization): TextVectorization layer for sentence tokenization.\n",
    "    vocab (list of str): Extracted vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define TextVectorization object with the appropriate standardize parameter\n",
    "    sentence_vectorizer = tf.keras.layers.TextVectorization(standardize=None)\n",
    "\n",
    "    # Adapt the sentence vectorization object to the given sentences\n",
    "    sentence_vectorizer.adapt(sentences)\n",
    "    \n",
    "    # Get the vocabulary\n",
    "    vocab = sentence_vectorizer.get_vocabulary()\n",
    " \n",
    "    return sentence_vectorizer, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Test vocab size: 29847\n"
     ]
    }
   ],
   "source": [
    "# Using the function to get the adapted vectorizer and vocabulary\n",
    "\n",
    "test_vectorizer, test_vocab = get_sentence_vectorizer(train_sentences)\n",
    "\n",
    "print(f\"Test vocab size: {len(test_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', '.', ',', 'in', 'of', 'to', 'a', 'and', 'The', \"'s\", 'for', 'has', 'is', 'on', 'that', 'with', 'have', 'said']\n"
     ]
    }
   ],
   "source": [
    "print(test_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like learning new NLP models !\n",
      "Sentence vectorized: [  654  1211  6896    69     1 11044  4126]\n"
     ]
    }
   ],
   "source": [
    "# Testing the vectorizer\n",
    "\n",
    "sentence = \"I like learning new NLP models !\"\n",
    "\n",
    "sentence_vectorized = test_vectorizer(sentence)\n",
    "\n",
    "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is a bit simpler than encoding the sentences, because there are only a few tags. Also, there will be one extra tag to represent the padded token that some sentences may have included. Padding will not interfere at all in this task.\n",
    "\n",
    "There is no meaning in having an UNK token for labels and also the padding token will be another number different from 0 for the labels. So, TextVectorization is not a good choice here.\n",
    "\n",
    "We will code our own label vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Labels: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[0]}\")\n",
    "print(f\"Labels: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function extract all the different tags in a given set of labels (all the training example set) and return us the uniqe ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(labels):\n",
    "    tag_set = set() # Define an empty set\n",
    "    for el in labels:\n",
    "        for tag in el.split(\" \"):\n",
    "            tag_set.add(tag)\n",
    "    tag_list = list(tag_set) \n",
    "    tag_list.sort()\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
     ]
    }
   ],
   "source": [
    "tags = get_tags(train_labels)\n",
    "print(len(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generate a **tag map**, i.e., a mapping between the tags and **positive** integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_map(tags):\n",
    "    tag_map = {}\n",
    "    for i,tag in enumerate(tags):\n",
    "        tag_map[tag] = i \n",
    "    return tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n"
     ]
    }
   ],
   "source": [
    "tag_map = make_tag_map(tags)\n",
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextVectorization has already padded the sentences, so we must ensure that the labels are properly padded as well.Tensorflow has built-in functions for padding `tf.keras.utils.pad_sequences`.\n",
    "\n",
    "We will pad the vectorized labels with the value -1. We will not use 0 to simplify loss masking and evaluation in further steps. \n",
    "\n",
    "This is because to properly classify one token, a log softmax transformation will be performed and the index with greater value will be the index label. Since index starts at 0, it is better to keep the label 0 as a valid index, even though it is possible to also use 0 as a mask value for labels, but it would require some tweaks in the model architecture or in the loss computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the label vectorizer.\n",
    "\n",
    "This function inputs a list of labels and a tag mapping and outputs their respective label ids via a tag map lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vectorizer(labels, tag_map):\n",
    "    \"\"\"\n",
    "    Convert list of label strings to padded label IDs using a tag mapping.\n",
    "\n",
    "    Parameters:\n",
    "    labels (list of str): List of label strings.\n",
    "    tag_map (dict): Dictionary mapping tags to IDs.\n",
    "    Returns:\n",
    "    label_ids (numpy.ndarray): Padded array of label IDs.\n",
    "    \"\"\"\n",
    "    label_ids = [] # It can't be a numpy array yet, since each sentence has a different size\n",
    "\n",
    "    # Each element in labels is a string of tags so for each of them:\n",
    "    for element in labels:\n",
    "        # Split it into single tokens. You may use .split function for strings. Be aware to split it by a blank space!\n",
    "        tokens = element.split()\n",
    "        \n",
    "        # DEBUG PRINT\n",
    "        print(tokens,\"\\n\")\n",
    "\n",
    "        # Use the dictionaty tag_map passed as an argument to the label_vectorizer function\n",
    "        # to make the correspondence between tags and numbers. \n",
    "        element_ids = []\n",
    "\n",
    "        for token in tokens:\n",
    "            # Tag map lookup\n",
    "            # Appending the ids corresponding to the tokens in the label\n",
    "            element_ids.append(tag_map[token])\n",
    "        \n",
    "        # DEBUG PRINT\n",
    "        print(element_ids,\"\\n\")\n",
    "\n",
    "        # Append the found ids to corresponding to the current element to label_ids list\n",
    "        label_ids.append(element_ids)\n",
    "\n",
    "    # DEBUG PRINT\n",
    "    print(label_ids,\"\\n\")\n",
    "        \n",
    "    # Pad the elements\n",
    "    label_ids = tf.keras.utils.pad_sequences(label_ids, padding='post', value=-1)\n",
    "    \n",
    "    # DEBUG PRINT\n",
    "    print(label_ids,\"\\n\")\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: They marched from the Houses of Parliament to a rally in Hyde Park .\n",
      "Labels: O O O O O O O O O O O B-geo I-geo O\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[2]}\")\n",
    "print(f\"Labels: {train_labels[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O'] \n",
      "\n",
      "[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 10, 16] \n",
      "\n",
      "[[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 10, 16]] \n",
      "\n",
      "[[16 16 16 16 16 16 16 16 16 16 16  2 10 16]] \n",
      "\n",
      "Vectorized labels: [[16 16 16 16 16 16 16 16 16 16 16  2 10 16]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vectorized labels: {label_vectorizer([train_labels[2]], tag_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at two examples together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['They marched from the Houses of Parliament to a rally in Hyde Park .'\n",
      " 'Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 .']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[2:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O O O O O O O O O O O B-geo I-geo O' 'O O O O O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels: {train_labels[2:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorized labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlabel_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mtag_map\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 16\u001b[0m, in \u001b[0;36mlabel_vectorizer\u001b[1;34m(labels, tag_map)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Each element in labels is a string of tags so for each of them:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Split it into single tokens. You may use .split function for strings. Be aware to split it by a blank space!\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# DEBUG PRINT\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "print(f\"Vectorized labels: {label_vectorizer([train_labels[2:4]], tag_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can process one label at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the dataset for training, validation and testing. \n",
    "\n",
    "We will be using `tf.data.Dataset` class, which provides an optimized way to handle data to feed into a tensorflow model. It avoids keeping all the data in memory, thus it makes the training faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
